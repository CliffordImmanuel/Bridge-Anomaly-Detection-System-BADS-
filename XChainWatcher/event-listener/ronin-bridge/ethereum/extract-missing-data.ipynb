{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import threading\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key():\n",
    "    return os.getenv('ETHEREUM_API_KEY')\n",
    "\n",
    "CONNECTION_URL = \"https://svc.blockdaemon.com/ethereum/mainnet/native\"\n",
    "OPTIONS = {\n",
    "    'headers': {\n",
    "        \"accept\": \"application/json\",\n",
    "        'x-api-key': get_api_key()\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_logs(from_block, to_block, topic, contract):\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getLogs\",\n",
    "        \"params\": [{\n",
    "            \"fromBlock\": hex(from_block),\n",
    "            \"toBlock\": hex(to_block),\n",
    "            \"address\": contract,\n",
    "            \"topics\": topic\n",
    "        }],\n",
    "        \"id\": 1\n",
    "    }\n",
    "\n",
    "    response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=payload)\n",
    "    response_json = response.json()\n",
    "    if 'result' in response_json:\n",
    "        return response_json['result']\n",
    "    else:\n",
    "        print(f\"Error fetching logs for blocks {from_block} to {to_block}: {response_json}\")\n",
    "        return []\n",
    "\n",
    "def save_logs(logs, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(logs, f)\n",
    "\n",
    "def extract_transaction_hashes(logs):\n",
    "    return set(log['transactionHash'] for log in logs)\n",
    "\n",
    "def save_hashes_to_csv(hashes, filename):\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['hash'])\n",
    "        for tx_hash in hashes:\n",
    "            writer.writerow([tx_hash])\n",
    "\n",
    "def main(from_block, to_block, folder_name, topic, contract):\n",
    "    block_range = 500\n",
    "    futures = []\n",
    "    total_logs_count = 0\n",
    "    merged_logs = []\n",
    "\n",
    "    try:\n",
    "\n",
    "        if not os.path.exists(f'./data-get-logs/{folder_name}/'):\n",
    "            os.makedirs(f'./data-get-logs/{folder_name}/')\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            for start in range(from_block, to_block, block_range):\n",
    "                end = min(start + block_range - 1, to_block)\n",
    "                future = executor.submit(get_logs, start, end, topic, contract)\n",
    "                futures.append((future, start, end))\n",
    "\n",
    "            for future, start, end in futures:\n",
    "                logs = future.result()\n",
    "                total_logs_count += len(logs)\n",
    "                merged_logs.extend(logs)\n",
    "\n",
    "        print(f\"Total logs fetched: {total_logs_count}\")\n",
    "\n",
    "        merged_logs_count = len(merged_logs)\n",
    "        with open(f'./data-get-logs/{folder_name}/merged_logs.json', 'w') as f:\n",
    "            json.dump(merged_logs, f)\n",
    "        \n",
    "        print(f\"Merged all log files into ./data/{folder_name}/merged_logs.json\")\n",
    "        print(f\"Total logs in merged file: {merged_logs_count}\")\n",
    "\n",
    "        # Extract transaction hashes and save to CSV\n",
    "        transaction_hashes = extract_transaction_hashes(merged_logs)\n",
    "        save_hashes_to_csv(transaction_hashes, f'./data-get-logs/{folder_name}/transaction_hashes.csv')\n",
    "\n",
    "        print(f\"Transaction hashes saved to ./data/{folder_name}/transaction_hashes.csv\")\n",
    "        print(f\"Total transaction hashes: {len(transaction_hashes)}\")\n",
    "    except Exception as e:\n",
    "        with open(f'./data-get-logs/{folder_name}/errors.txt', 'w') as f:\n",
    "            f.write(str(e))\n",
    "            f.write(merged_logs)\n",
    "        print(f\"Error!! Saved to ./data-get-logs/{folder_name}/errors.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant Block Numbers**\n",
    "\n",
    " # September 13, 2021 00:00:00\n",
    "\n",
    "13916166 # January 1, 2022 00:00:00\n",
    "\n",
    "14673143 # April 28, 2022 13:32:41\n",
    "\n",
    "20429463 # July 31, 2024 10:17:35\n",
    "\n",
    "**Relevant Topics**\n",
    "TokenWithdrew(uint256,address,address,address,uint32,uint256): 0x86174ea401f083b9bb1bdebca3068f27fb023c7091365ed2a8a02b8d75cf0e52\n",
    "\n",
    "TokenDeposited(uint256,address,address,uint256): 0x72848855a2461abf0dd243723dfcc9163eec2ea5215469d101c0d9c9ef58940d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all log files into ./data/logs-01Jan-28Apr-withds/merged_logs.json\n",
      "Total logs fetched: 25470\n",
      "Total logs in merged file: 25470\n",
      "Transaction hashes saved to ./data/logs-01Jan-28Apr-withds/transaction_hashes.csv\n",
      "Total transaction hashes: 25470\n"
     ]
    }
   ],
   "source": [
    "from_block = 13916166 # 1 jan\n",
    "to_block = 14673143 # 28 Apr\n",
    "topics = [\"0x86174ea401f083b9bb1bdebca3068f27fb023c7091365ed2a8a02b8d75cf0e52\"]\n",
    "contract = \"0x1A2a1c938CE3eC39b6D47113c7955bAa9DD454F2\"\n",
    "main(from_block, to_block, \"logs-01Jan-28Apr-withds\", topics, contract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all log files into ./data/logs-01Jan-28Apr-deps/merged_logs.json\n",
      "Total logs fetched: 43989\n",
      "Total logs in merged file: 43989\n",
      "Transaction hashes saved to ./data/logs-01Jan-28Apr-deps/transaction_hashes.csv\n",
      "Total transaction hashes: 43989\n"
     ]
    }
   ],
   "source": [
    "from_block = 13916166 # 1 jan\n",
    "to_block = 14673143 # 28 Apr\n",
    "topics = [\"0x72848855a2461abf0dd243723dfcc9163eec2ea5215469d101c0d9c9ef58940d\"]\n",
    "contract = \"0x1A2a1c938CE3eC39b6D47113c7955bAa9DD454F2\"\n",
    "main(from_block, to_block, \"logs-01Jan-28Apr-deps\", topics, contract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total logs fetched: 58037\n",
      "Merged all log files into ./data/logs-28Apr-today/merged_logs.json\n",
      "Total logs in merged file: 58037\n",
      "Transaction hashes saved to ./data/logs-28Apr-today/transaction_hashes.csv\n",
      "Total transaction hashes: 58037\n"
     ]
    }
   ],
   "source": [
    "from_block = 14673144 # 28 Apr\n",
    "to_block = 20429463 # 31 Jul\n",
    "contract = \"0x64192819Ac13Ef72bF6b5AE239AC672B43a9AF08\"\n",
    "topics = [\"0x21e88e956aa3e086f6388e899965cef814688f99ad8bb29b08d396571016372d\"] #only withdrawals\n",
    "main(from_block, to_block, \"logs-28Apr-today\", topics, contract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Transaction Receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "RATE_LIMIT = 40  # requests per second\n",
    "REQUEST_INTERVAL = 1 / RATE_LIMIT\n",
    "\n",
    "def fetch_receipt(tx_hash, folder_name):\n",
    "    data = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getTransactionReceipt\",\n",
    "        \"params\": [tx_hash]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=data)\n",
    "        response_json = response.json()\n",
    "        if 'result' in response_json:\n",
    "            return response_json['result']\n",
    "        else:\n",
    "            print(f\"Error fetching tx receipt for tx {tx_hash}, {response_json}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        with open(f\"./data-get-logs/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving transaction: {tx_hash}, {e}\\n\")\n",
    "\n",
    "def process_hashes(hashes, folder_name):\n",
    "    receipts = []\n",
    "    total_hashes = len(hashes)\n",
    "    progress_interval = max(1, total_hashes // 100)  # Update progress every 1%\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=RATE_LIMIT) as executor:\n",
    "        futures = {executor.submit(fetch_receipt, tx_hash, folder_name): tx_hash for tx_hash in hashes}\n",
    "        \n",
    "        for idx, future in enumerate(as_completed(futures), 1):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                receipts.append(result)\n",
    "\n",
    "                if idx % progress_interval == 0 or idx == total_hashes:\n",
    "                    progress_percentage = (idx / total_hashes) * 100\n",
    "                    print(f\"Progress: {progress_percentage:.2f}% ({idx}/{total_hashes})\")\n",
    "        \n",
    "                time.sleep(REQUEST_INTERVAL)\n",
    "            except Exception as e:\n",
    "                with open(f\"./data-get-logs/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "                    error_file.write(f\"Error retrieving transaction: {futures[future]}, {e}\\n\")\n",
    "    return receipts\n",
    "\n",
    "def retrieve_receipts(folder_name):\n",
    "    # Read hashes from CSV\n",
    "    hashes_df = pd.read_csv(f'./data-get-logs/{folder_name}/transaction_hashes.csv')\n",
    "    hashes = hashes_df['hash'].drop_duplicates().tolist()\n",
    "\n",
    "    # Fetch receipts\n",
    "    receipts = process_hashes(hashes, folder_name)\n",
    "\n",
    "    # Save receipts to JSON\n",
    "    with open(f'./data-get-logs/{folder_name}/tx_receipts.json', 'w') as f:\n",
    "        json.dump(receipts, f)\n",
    "\n",
    "    # Print the count of processed receipts\n",
    "    print(f\"Total receipts fetched: {len(receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (254/25470)\n",
      "Progress: 1.99% (508/25470)\n",
      "Progress: 2.99% (762/25470)\n",
      "Progress: 3.99% (1016/25470)\n",
      "Progress: 4.99% (1270/25470)\n",
      "Progress: 5.98% (1524/25470)\n",
      "Progress: 6.98% (1778/25470)\n",
      "Progress: 7.98% (2032/25470)\n",
      "Progress: 8.98% (2286/25470)\n",
      "Progress: 9.97% (2540/25470)\n",
      "Progress: 10.97% (2794/25470)\n",
      "Progress: 11.97% (3048/25470)\n",
      "Progress: 12.96% (3302/25470)\n",
      "Progress: 13.96% (3556/25470)\n",
      "Progress: 14.96% (3810/25470)\n",
      "Progress: 15.96% (4064/25470)\n",
      "Progress: 16.95% (4318/25470)\n",
      "Progress: 17.95% (4572/25470)\n",
      "Progress: 18.95% (4826/25470)\n",
      "Progress: 19.95% (5080/25470)\n",
      "Progress: 20.94% (5334/25470)\n",
      "Progress: 21.94% (5588/25470)\n",
      "Progress: 22.94% (5842/25470)\n",
      "Progress: 23.93% (6096/25470)\n",
      "Progress: 24.93% (6350/25470)\n",
      "Progress: 25.93% (6604/25470)\n",
      "Progress: 26.93% (6858/25470)\n",
      "Progress: 27.92% (7112/25470)\n",
      "Progress: 28.92% (7366/25470)\n",
      "Progress: 29.92% (7620/25470)\n",
      "Progress: 30.91% (7874/25470)\n",
      "Progress: 31.91% (8128/25470)\n",
      "Progress: 32.91% (8382/25470)\n",
      "Progress: 33.91% (8636/25470)\n",
      "Progress: 34.90% (8890/25470)\n",
      "Progress: 35.90% (9144/25470)\n",
      "Progress: 36.90% (9398/25470)\n",
      "Progress: 37.90% (9652/25470)\n",
      "Progress: 38.89% (9906/25470)\n",
      "Progress: 39.89% (10160/25470)\n",
      "Progress: 40.89% (10414/25470)\n",
      "Progress: 41.88% (10668/25470)\n",
      "Progress: 42.88% (10922/25470)\n",
      "Progress: 43.88% (11176/25470)\n",
      "Progress: 44.88% (11430/25470)\n",
      "Progress: 45.87% (11684/25470)\n",
      "Progress: 46.87% (11938/25470)\n",
      "Progress: 47.87% (12192/25470)\n",
      "Progress: 48.87% (12446/25470)\n",
      "Progress: 49.86% (12700/25470)\n",
      "Progress: 50.86% (12954/25470)\n",
      "Progress: 51.86% (13208/25470)\n",
      "Progress: 52.85% (13462/25470)\n",
      "Progress: 53.85% (13716/25470)\n",
      "Progress: 54.85% (13970/25470)\n",
      "Progress: 55.85% (14224/25470)\n",
      "Progress: 56.84% (14478/25470)\n",
      "Progress: 57.84% (14732/25470)\n",
      "Progress: 58.84% (14986/25470)\n",
      "Progress: 59.84% (15240/25470)\n",
      "Progress: 60.83% (15494/25470)\n",
      "Progress: 61.83% (15748/25470)\n",
      "Progress: 62.83% (16002/25470)\n",
      "Progress: 63.82% (16256/25470)\n",
      "Progress: 64.82% (16510/25470)\n",
      "Progress: 65.82% (16764/25470)\n",
      "Progress: 66.82% (17018/25470)\n",
      "Progress: 67.81% (17272/25470)\n",
      "Progress: 68.81% (17526/25470)\n",
      "Progress: 69.81% (17780/25470)\n",
      "Progress: 70.80% (18034/25470)\n",
      "Progress: 71.80% (18288/25470)\n",
      "Progress: 72.80% (18542/25470)\n",
      "Progress: 73.80% (18796/25470)\n",
      "Progress: 74.79% (19050/25470)\n",
      "Progress: 75.79% (19304/25470)\n",
      "Progress: 76.79% (19558/25470)\n",
      "Progress: 77.79% (19812/25470)\n",
      "Progress: 78.78% (20066/25470)\n",
      "Progress: 79.78% (20320/25470)\n",
      "Progress: 80.78% (20574/25470)\n",
      "Progress: 81.77% (20828/25470)\n",
      "Progress: 82.77% (21082/25470)\n",
      "Progress: 83.77% (21336/25470)\n",
      "Progress: 84.77% (21590/25470)\n",
      "Progress: 85.76% (21844/25470)\n",
      "Progress: 86.76% (22098/25470)\n",
      "Progress: 87.76% (22352/25470)\n",
      "Progress: 88.76% (22606/25470)\n",
      "Progress: 89.75% (22860/25470)\n",
      "Progress: 90.75% (23114/25470)\n",
      "Progress: 91.75% (23368/25470)\n",
      "Progress: 92.74% (23622/25470)\n",
      "Progress: 93.74% (23876/25470)\n",
      "Progress: 94.74% (24130/25470)\n",
      "Progress: 95.74% (24384/25470)\n",
      "Progress: 96.73% (24638/25470)\n",
      "Progress: 97.73% (24892/25470)\n",
      "Progress: 98.73% (25146/25470)\n",
      "Progress: 99.73% (25400/25470)\n",
      "Progress: 100.00% (25470/25470)\n",
      "Total receipts fetched: 25470\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-01Jan-28Apr-withds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (439/43989)\n",
      "Progress: 2.00% (878/43989)\n",
      "Progress: 2.99% (1317/43989)\n",
      "Progress: 3.99% (1756/43989)\n",
      "Progress: 4.99% (2195/43989)\n",
      "Progress: 5.99% (2634/43989)\n",
      "Progress: 6.99% (3073/43989)\n",
      "Progress: 7.98% (3512/43989)\n",
      "Progress: 8.98% (3951/43989)\n",
      "Progress: 9.98% (4390/43989)\n",
      "Progress: 10.98% (4829/43989)\n",
      "Progress: 11.98% (5268/43989)\n",
      "Progress: 12.97% (5707/43989)\n",
      "Progress: 13.97% (6146/43989)\n",
      "Progress: 14.97% (6585/43989)\n",
      "Progress: 15.97% (7024/43989)\n",
      "Progress: 16.97% (7463/43989)\n",
      "Progress: 17.96% (7902/43989)\n",
      "Progress: 18.96% (8341/43989)\n",
      "Progress: 19.96% (8780/43989)\n",
      "Progress: 20.96% (9219/43989)\n",
      "Progress: 21.96% (9658/43989)\n",
      "Progress: 22.95% (10097/43989)\n",
      "Progress: 23.95% (10536/43989)\n",
      "Progress: 24.95% (10975/43989)\n",
      "Progress: 25.95% (11414/43989)\n",
      "Progress: 26.95% (11853/43989)\n",
      "Progress: 27.94% (12292/43989)\n",
      "Progress: 28.94% (12731/43989)\n",
      "Progress: 29.94% (13170/43989)\n",
      "Progress: 30.94% (13609/43989)\n",
      "Progress: 31.94% (14048/43989)\n",
      "Progress: 32.93% (14487/43989)\n",
      "Progress: 33.93% (14926/43989)\n",
      "Progress: 34.93% (15365/43989)\n",
      "Progress: 35.93% (15804/43989)\n",
      "Progress: 36.93% (16243/43989)\n",
      "Progress: 37.92% (16682/43989)\n",
      "Progress: 38.92% (17121/43989)\n",
      "Progress: 39.92% (17560/43989)\n",
      "Progress: 40.92% (17999/43989)\n",
      "Progress: 41.92% (18438/43989)\n",
      "Progress: 42.91% (18877/43989)\n",
      "Progress: 43.91% (19316/43989)\n",
      "Progress: 44.91% (19755/43989)\n",
      "Progress: 45.91% (20194/43989)\n",
      "Progress: 46.90% (20633/43989)\n",
      "Progress: 47.90% (21072/43989)\n",
      "Progress: 48.90% (21511/43989)\n",
      "Progress: 49.90% (21950/43989)\n",
      "Progress: 50.90% (22389/43989)\n",
      "Progress: 51.89% (22828/43989)\n",
      "Progress: 52.89% (23267/43989)\n",
      "Progress: 53.89% (23706/43989)\n",
      "Progress: 54.89% (24145/43989)\n",
      "Progress: 55.89% (24584/43989)\n",
      "Progress: 56.88% (25023/43989)\n",
      "Progress: 57.88% (25462/43989)\n",
      "Progress: 58.88% (25901/43989)\n",
      "Progress: 59.88% (26340/43989)\n",
      "Progress: 60.88% (26779/43989)\n",
      "Progress: 61.87% (27218/43989)\n",
      "Progress: 62.87% (27657/43989)\n",
      "Progress: 63.87% (28096/43989)\n",
      "Progress: 64.87% (28535/43989)\n",
      "Progress: 65.87% (28974/43989)\n",
      "Progress: 66.86% (29413/43989)\n",
      "Progress: 67.86% (29852/43989)\n",
      "Progress: 68.86% (30291/43989)\n",
      "Progress: 69.86% (30730/43989)\n",
      "Progress: 70.86% (31169/43989)\n",
      "Progress: 71.85% (31608/43989)\n",
      "Progress: 72.85% (32047/43989)\n",
      "Progress: 73.85% (32486/43989)\n",
      "Progress: 74.85% (32925/43989)\n",
      "Progress: 75.85% (33364/43989)\n",
      "Progress: 76.84% (33803/43989)\n",
      "Progress: 77.84% (34242/43989)\n",
      "Progress: 78.84% (34681/43989)\n",
      "Progress: 79.84% (35120/43989)\n",
      "Progress: 80.84% (35559/43989)\n",
      "Progress: 81.83% (35998/43989)\n",
      "Progress: 82.83% (36437/43989)\n",
      "Progress: 83.83% (36876/43989)\n",
      "Progress: 84.83% (37315/43989)\n",
      "Progress: 85.83% (37754/43989)\n",
      "Progress: 86.82% (38193/43989)\n",
      "Progress: 87.82% (38632/43989)\n",
      "Progress: 88.82% (39071/43989)\n",
      "Progress: 89.82% (39510/43989)\n",
      "Progress: 90.82% (39949/43989)\n",
      "Progress: 91.81% (40388/43989)\n",
      "Progress: 92.81% (40827/43989)\n",
      "Progress: 93.81% (41266/43989)\n",
      "Progress: 94.81% (41705/43989)\n",
      "Progress: 95.81% (42144/43989)\n",
      "Progress: 96.80% (42583/43989)\n",
      "Progress: 97.80% (43022/43989)\n",
      "Progress: 98.80% (43461/43989)\n",
      "Progress: 99.80% (43900/43989)\n",
      "Progress: 100.00% (43989/43989)\n",
      "Total receipts fetched: 43989\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-01Jan-28Apr-deps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (580/58037)\n",
      "Progress: 2.00% (1160/58037)\n",
      "Progress: 3.00% (1740/58037)\n",
      "Progress: 4.00% (2320/58037)\n",
      "Progress: 5.00% (2900/58037)\n",
      "Progress: 6.00% (3480/58037)\n",
      "Progress: 7.00% (4060/58037)\n",
      "Progress: 7.99% (4640/58037)\n",
      "Progress: 8.99% (5220/58037)\n",
      "Progress: 9.99% (5800/58037)\n",
      "Progress: 10.99% (6380/58037)\n",
      "Progress: 11.99% (6960/58037)\n",
      "Progress: 12.99% (7540/58037)\n",
      "Progress: 13.99% (8120/58037)\n",
      "Progress: 14.99% (8700/58037)\n",
      "Progress: 15.99% (9280/58037)\n",
      "Progress: 16.99% (9860/58037)\n",
      "Progress: 17.99% (10440/58037)\n",
      "Progress: 18.99% (11020/58037)\n",
      "Progress: 19.99% (11600/58037)\n",
      "Progress: 20.99% (12180/58037)\n",
      "Progress: 21.99% (12760/58037)\n",
      "Progress: 22.99% (13340/58037)\n",
      "Progress: 23.98% (13920/58037)\n",
      "Progress: 24.98% (14500/58037)\n",
      "Progress: 25.98% (15080/58037)\n",
      "Progress: 26.98% (15660/58037)\n",
      "Progress: 27.98% (16240/58037)\n",
      "Progress: 28.98% (16820/58037)\n",
      "Progress: 29.98% (17400/58037)\n",
      "Progress: 30.98% (17980/58037)\n",
      "Progress: 31.98% (18560/58037)\n",
      "Progress: 32.98% (19140/58037)\n",
      "Progress: 33.98% (19720/58037)\n",
      "Progress: 34.98% (20300/58037)\n",
      "Progress: 35.98% (20880/58037)\n",
      "Progress: 36.98% (21460/58037)\n",
      "Progress: 37.98% (22040/58037)\n",
      "Progress: 38.98% (22620/58037)\n",
      "Progress: 39.97% (23200/58037)\n",
      "Progress: 40.97% (23780/58037)\n",
      "Progress: 41.97% (24360/58037)\n",
      "Progress: 42.97% (24940/58037)\n",
      "Progress: 43.97% (25520/58037)\n",
      "Progress: 44.97% (26100/58037)\n",
      "Progress: 45.97% (26680/58037)\n",
      "Progress: 46.97% (27260/58037)\n",
      "Progress: 47.97% (27840/58037)\n",
      "Progress: 48.97% (28420/58037)\n",
      "Progress: 49.97% (29000/58037)\n",
      "Progress: 50.97% (29580/58037)\n",
      "Progress: 51.97% (30160/58037)\n",
      "Progress: 52.97% (30740/58037)\n",
      "Progress: 53.97% (31320/58037)\n",
      "Progress: 54.96% (31900/58037)\n",
      "Progress: 55.96% (32480/58037)\n",
      "Progress: 56.96% (33060/58037)\n",
      "Progress: 57.96% (33640/58037)\n",
      "Progress: 58.96% (34220/58037)\n",
      "Progress: 59.96% (34800/58037)\n",
      "Progress: 60.96% (35380/58037)\n",
      "Progress: 61.96% (35960/58037)\n",
      "Progress: 62.96% (36540/58037)\n",
      "Progress: 63.96% (37120/58037)\n",
      "Progress: 64.96% (37700/58037)\n",
      "Progress: 65.96% (38280/58037)\n",
      "Progress: 66.96% (38860/58037)\n",
      "Progress: 67.96% (39440/58037)\n",
      "Progress: 68.96% (40020/58037)\n",
      "Progress: 69.96% (40600/58037)\n",
      "Progress: 70.95% (41180/58037)\n",
      "Progress: 71.95% (41760/58037)\n",
      "Progress: 72.95% (42340/58037)\n",
      "Progress: 73.95% (42920/58037)\n",
      "Progress: 74.95% (43500/58037)\n",
      "Progress: 75.95% (44080/58037)\n",
      "Progress: 76.95% (44660/58037)\n",
      "Progress: 77.95% (45240/58037)\n",
      "Progress: 78.95% (45820/58037)\n",
      "Progress: 79.95% (46400/58037)\n",
      "Progress: 80.95% (46980/58037)\n",
      "Progress: 81.95% (47560/58037)\n",
      "Progress: 82.95% (48140/58037)\n",
      "Progress: 83.95% (48720/58037)\n",
      "Progress: 84.95% (49300/58037)\n",
      "Progress: 85.95% (49880/58037)\n",
      "Progress: 86.94% (50460/58037)\n",
      "Progress: 87.94% (51040/58037)\n",
      "Progress: 88.94% (51620/58037)\n",
      "Progress: 89.94% (52200/58037)\n",
      "Progress: 90.94% (52780/58037)\n",
      "Progress: 91.94% (53360/58037)\n",
      "Progress: 92.94% (53940/58037)\n",
      "Progress: 93.94% (54520/58037)\n",
      "Progress: 94.94% (55100/58037)\n",
      "Progress: 95.94% (55680/58037)\n",
      "Progress: 96.94% (56260/58037)\n",
      "Progress: 97.94% (56840/58037)\n",
      "Progress: 98.94% (57420/58037)\n",
      "Progress: 99.94% (58000/58037)\n",
      "Progress: 100.00% (58037/58037)\n",
      "Total receipts fetched: 58037\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts('logs-28Apr-today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def load_failed_hashes(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        hashes = [re.findall(pattern='0x[a-fA-F0-9]{64}', string=line)[0] for line in f]\n",
    "    return hashes\n",
    "\n",
    "def retrieve_failed_receipts(folder_name):\n",
    "    errors_file = f'./data-get-logs/{folder_name}/errors.txt'\n",
    "    if not os.path.exists(errors_file):\n",
    "        print(f\"Error file {errors_file} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Load failed transaction hashes from errors.txt\n",
    "    failed_hashes = load_failed_hashes(f'./data-get-logs/{folder_name}/errors.txt')\n",
    "\n",
    "    # Fetch receipts for failed hashes\n",
    "    failed_receipts = process_hashes(failed_hashes, folder_name)\n",
    "\n",
    "    # Save failed receipts to JSON\n",
    "    with open(f'./data-get-logs/{folder_name}/tx_receipts_2.json', 'w') as f:\n",
    "        json.dump(failed_receipts, f)\n",
    "\n",
    "    # Print the count of processed failed receipts\n",
    "    print(f\"Total failed receipts fetched: {len(failed_receipts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0x48bfbc5d66396e2eab1bdbac59c60e2a817e2f919ffce13d7cefc86aac78f7b3']\n",
      "Progress: 100.00% (1/1)\n",
      "Total failed receipts fetched: 1\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 25.00% (1/4)\n",
      "Progress: 50.00% (2/4)\n",
      "Progress: 75.00% (3/4)\n",
      "Progress: 100.00% (4/4)\n",
      "Total failed receipts fetched: 4\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 14.29% (1/7)\n",
      "Progress: 28.57% (2/7)\n",
      "Progress: 42.86% (3/7)\n",
      "Progress: 57.14% (4/7)\n",
      "Progress: 71.43% (5/7)\n",
      "Progress: 85.71% (6/7)\n",
      "Progress: 100.00% (7/7)\n",
      "Total failed receipts fetched: 7\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-28Apr-today')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second interval, we need to merge the json files with transaction receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_json_files(file1, file2, output_file):\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        json1 = json.load(f1)\n",
    "        json2 = json.load(f2)\n",
    "\n",
    "    merged_json = json1 + json2\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_json, f)\n",
    "\n",
    "    count1 = len(json1)\n",
    "    count2 = len(json2)\n",
    "    count_merged = len(merged_json)\n",
    "\n",
    "    print(f\"Number of transaction receipts in {file1}: {count1}\")\n",
    "    print(f\"Number of transaction receipts in {file2}: {count2}\")\n",
    "    print(f\"Number of transaction receipts in {output_file}: {count_merged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in tx_receipts: 43989\n",
      "Number of transaction receipts in tx_receipts_2: 1\n",
      "Number of transaction receipts in the merged file: 43990\n"
     ]
    }
   ],
   "source": [
    "file1 = f'./data-get-logs/logs-01Jan-28Apr-deps/tx_receipts.json'\n",
    "file2 = f'./data-get-logs/logs-01Jan-28Apr-deps/tx_receipts_2.json'\n",
    "output_file = f'./data-get-logs/logs-01Jan-28Apr-deps/tx_receipts.json'\n",
    "merge_json_files(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in tx_receipts: 25470\n",
      "Number of transaction receipts in tx_receipts_2: 4\n",
      "Number of transaction receipts in the merged file: 25474\n"
     ]
    }
   ],
   "source": [
    "file1 = f'./data-get-logs/logs-01Jan-28Apr-withds/tx_receipts.json'\n",
    "file2 = f'./data-get-logs/logs-01Jan-28Apr-withds/tx_receipts_2.json'\n",
    "output_file = f'./data-get-logs/logs-01Jan-28Apr-withds/tx_receipts.json'\n",
    "merge_json_files(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data-get-logs/logs-28Apr-today/tx_receipts.json: 58030\n",
      "Number of transaction receipts in ./data-get-logs/logs-28Apr-today/tx_receipts_2.json: 7\n",
      "Number of transaction receipts in ./data-get-logs/logs-28Apr-today/tx_receipts.json: 58037\n"
     ]
    }
   ],
   "source": [
    "file1 = f'./data-get-logs/logs-28Apr-today/tx_receipts.json'\n",
    "file2 = f'./data-get-logs/logs-28Apr-today/tx_receipts_2.json'\n",
    "output_file = f'./data-get-logs/logs-28Apr-today/tx_receipts.json'\n",
    "merge_json_files(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from json file with result field of receipts\n",
    "def remove_duplicates(folder_name, filename):\n",
    "    file = f'{folder_name}/{filename}'\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        receipts = json.load(f)\n",
    "\n",
    "    unique_receipts = []\n",
    "    unique_hashes = set()\n",
    "\n",
    "    for receipt in receipts:\n",
    "        tx_hash = receipt.get('transactionHash')\n",
    "        if tx_hash not in unique_hashes:\n",
    "            unique_hashes.add(tx_hash)\n",
    "            unique_receipts.append(receipt)\n",
    "\n",
    "    # Save unique receipts to a new file\n",
    "    unique_output_file = f'{folder_name}/unique_tx_receipts.json'\n",
    "    with open(unique_output_file, 'w') as f:\n",
    "        json.dump(unique_receipts, f)\n",
    "\n",
    "    print(f\"Unique receipts saved to {unique_output_file}\")\n",
    "\n",
    "    print(f\"Removed {len(receipts) - len(unique_receipts)} duplicate receipts\")\n",
    "    print(f\"Total receipts: {len(receipts)}\")\n",
    "    print(f\"Total unique receipts: {len(unique_receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data-get-logs/logs-01Jan-28Apr-deps/unique_tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 43989\n",
      "Total unique receipts: 43989\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(\"./data-get-logs/logs-01Jan-28Apr-deps\", \"tx_receipts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data-get-logs/logs-01Jan-28Apr-withds/unique_tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 25470\n",
      "Total unique receipts: 25470\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(\"./data-get-logs/logs-01Jan-28Apr-withds\", \"tx_receipts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data-get-logs/logs-28Apr-today/unique_tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 58037\n",
      "Total unique receipts: 58037\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates(\"./data-get-logs/logs-28Apr-today\", \"tx_receipts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key():\n",
    "    return os.getenv('ETHEREUM_API_KEY')\n",
    "\n",
    "CONNECTION_URL = \"https://svc.blockdaemon.com/ethereum/mainnet/native\"\n",
    "\n",
    "\n",
    "OPTIONS = {\n",
    "    \"headers\": {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-Key\": get_api_key()\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_block_data(block_number, errors_file):\n",
    "    payload = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"params\": [],\n",
    "        \"method\": \"eth_getBlockByNumber\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        payload['params'] = [block_number, False]\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            block = response.json()[\"result\"]\n",
    "            block_no = int(block[\"number\"], 16)\n",
    "            timestamp = int(block[\"timestamp\"], 16)\n",
    "            transactions = len(block[\"transactions\"])\n",
    "            return f\"{block_no},{transactions},{timestamp}\\n\"\n",
    "        else:\n",
    "            with open(errors_file, \"a\") as error_file:\n",
    "                error_file.write(f\"Error code: {block_number}\\n\")\n",
    "            return \"null,null,null\\n\"\n",
    "    except Exception as e:\n",
    "        with open(errors_file, \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving block: {block_number}, {e}\\n\")\n",
    "        return \"null,null,null\\n\"\n",
    "\n",
    "def get_blocks_data(folder_name):\n",
    "    input_file = f'./data-get-logs/{folder_name}/tx_receipts.json'\n",
    "    output_file = f'./data-get-logs/{folder_name}/blocks.csv'\n",
    "    errors_file = f'./data-get-logs/{folder_name}/errors.txt'\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    print(f\"Extracting block number and Unix timestamp from {len(tx_receipts)} transaction receipts...\")\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts]\n",
    "\n",
    "    print(f\"Extracted {len(block_numbers)} block numbers...\")\n",
    "\n",
    "    with open(output_file, \"a\") as blocks_file:\n",
    "        blocks_file.write(\"block_number,transactions,timestamp\\n\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            # Submit tasks for each block in the range\n",
    "            futures = {executor.submit(get_block_data, block_number, errors_file): block_number for block_number in block_numbers if block_number is not None}\n",
    "\n",
    "            # Process the completed tasks and write to the file\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                block_number = futures[future]\n",
    "                try:\n",
    "                    blocks_data = future.result()\n",
    "                    blocks_file.write(blocks_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing block {block_number}: {e}\")\n",
    "\n",
    "    print(f'Extracted block number and Unix timestamp to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 43989 transaction receipts...\n",
      "Extracted 43989 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data-get-logs/logs-01Jan-28Apr-deps/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 25470 transaction receipts...\n",
      "Extracted 25470 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data-get-logs/logs-01Jan-28Apr-withds/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 58037 transaction receipts...\n",
      "Extracted 58037 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data-get-logs/logs-28Apr-today/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-28Apr-today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_missing_blocks(folder_name):\n",
    "    blocks_file = f'./data-get-logs/{folder_name}/blocks.csv'\n",
    "    receipts_file = f'./data-get-logs/{folder_name}/tx_receipts.json'\n",
    "    errors_file = f'./data-get-logs/{folder_name}/errors_2.txt'\n",
    "\n",
    "    blocks_data = pd.read_csv(blocks_file)\n",
    "\n",
    "    with open(receipts_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts]\n",
    "    \n",
    "    print(\"Loaded block numbers and block data...\")\n",
    "\n",
    "    print(len(block_numbers))\n",
    "    print(len(blocks_data['block_number']))\n",
    "    \n",
    "    missing_blocks = set(block_numbers) - set(hex(block) for block in blocks_data['block_number'])\n",
    "\n",
    "    print(f\"Total missing blocks: {len(missing_blocks)}\")\n",
    "\n",
    "    for missing_block in missing_blocks:\n",
    "        line = get_block_data(missing_block, errors_file)\n",
    "        with open(blocks_file, 'a') as blocks_file:\n",
    "            blocks_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "43989\n",
      "43989\n",
      "Total missing blocks: 0\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-01Jan-28Apr-deps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "25470\n",
      "25468\n",
      "Total missing blocks: 1\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-01Jan-28Apr-withds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "58037\n",
      "58036\n",
      "Total missing blocks: 1\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-28Apr-today')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with other data from Blockdaemon:\n",
    "\n",
    "### logs-01Jan-28Apr-withds logs-01Jan-28Apr-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data-get-logs/logs-01Jan-28Apr-withds/unique_tx_receipts.json: 25470\n",
      "Number of transaction receipts in ./data-get-logs/logs-01Jan-28Apr-deps/unique_tx_receipts.json: 43989\n",
      "Number of transaction receipts in ./merged-data/logs-01Jan-28Apr/unique_tx_receipts-rpc.json: 69459\n"
     ]
    }
   ],
   "source": [
    "# firstly merge the ones in this script\n",
    "file1 = './data-get-logs/logs-01Jan-28Apr-withds/unique_tx_receipts.json'\n",
    "file2 = './data-get-logs/logs-01Jan-28Apr-deps/unique_tx_receipts.json'\n",
    "output_file = './merged-data/logs-01Jan-28Apr/unique_tx_receipts-rpc.json'\n",
    "merge_json_files(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./merged-data/logs-01Jan-28Apr/unique_tx_receipts-rpc.json: 69459\n",
      "Number of transaction receipts in ./data-bd/logs-01Jan-28Apr/unique_tx_receipts.json: 53601\n",
      "Number of transaction receipts in ./merged-data/logs-01Jan-28Apr/unique_tx_receipts-merged.json: 123060\n",
      "Unique receipts saved to ./merged-data/logs-01Jan-28Apr/unique_tx_receipts.json\n",
      "Removed 50240 duplicate receipts\n",
      "Total receipts: 123060\n",
      "Total unique receipts: 72820\n"
     ]
    }
   ],
   "source": [
    "file1 = './merged-data/logs-01Jan-28Apr/unique_tx_receipts-rpc.json'\n",
    "file2 = './data-bd/logs-01Jan-28Apr/unique_tx_receipts.json'\n",
    "output_file = './merged-data/logs-01Jan-28Apr/unique_tx_receipts-merged.json'\n",
    "merge_json_files(file1, file2, output_file)\n",
    "# delete file 1\n",
    "os.remove(file1)\n",
    "\n",
    "remove_duplicates('./merged-data/logs-01Jan-28Apr', \"unique_tx_receipts-merged.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions left: 19219\n"
     ]
    }
   ],
   "source": [
    "file1 = './data-bd/logs-01Jan-28Apr/unique_tx_receipts.json'\n",
    "\n",
    "file2 = './merged-data/logs-01Jan-28Apr/unique_tx_receipts.json'\n",
    "\n",
    "#open each one\n",
    "# for each receipt in file2, check if it is in file1\n",
    "# if not, add to list\n",
    "# save json to file in merged-data\n",
    "output_file = './merged-data/logs-01Jan-28Apr/left_txs.json'\n",
    "\n",
    "with open(file1, 'r') as f:\n",
    "    receipts1 = json.load(f)\n",
    "    already_loaded_hashes = set([r.get('transactionHash') for r in receipts1])\n",
    "\n",
    "with open(file2, 'r') as f:\n",
    "    receipts2 = json.load(f)\n",
    "\n",
    "left_txs = []\n",
    "\n",
    "for receipt in receipts2:\n",
    "    tx_hash = receipt.get('transactionHash')\n",
    "    if tx_hash not in already_loaded_hashes:\n",
    "        left_txs.append(receipt)\n",
    "\n",
    "print(f\"Total transactions left: {len(left_txs)}\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(left_txs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logs-28Apr-today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data-bd/logs-28Apr-today/withdrawal_receipts.json: 19482\n",
      "Number of transaction receipts in ./data-get-logs/logs-28Apr-today/unique_tx_receipts.json: 58037\n",
      "Number of transaction receipts in ./merged-data/logs-28Apr-today/tx_receipts-merged.json: 77519\n"
     ]
    }
   ],
   "source": [
    "# Merge withdrawal receipts from data-bd/logs-28Apr-today with the unique_tx_receipts.json file in merged-data/logs-28Apr-today\n",
    "file1 = './data-bd/logs-28Apr-today/withdrawal_receipts.json'\n",
    "file2 = './data-get-logs/logs-28Apr-today/unique_tx_receipts.json'\n",
    "output_file = './merged-data/logs-28Apr-today/tx_receipts-merged.json'\n",
    "merge_json_files(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./merged-data/logs-28Apr-today/unique_tx_receipts.json\n",
      "Removed 19482 duplicate receipts\n",
      "Total receipts: 77519\n",
      "Total unique receipts: 58037\n"
     ]
    }
   ],
   "source": [
    "remove_duplicates('./merged-data/logs-28Apr-today', 'tx_receipts-merged.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transactions left: 38555\n"
     ]
    }
   ],
   "source": [
    "file1 = './data-bd/logs-28Apr-today/withdrawal_receipts.json'\n",
    "\n",
    "file2 = './merged-data/logs-28Apr-today/unique_tx_receipts.json'\n",
    "\n",
    "output_file = './merged-data/logs-28Apr-today/left_txs.json'\n",
    "\n",
    "with open(file1, 'r') as f:\n",
    "    receipts1 = json.load(f)\n",
    "    already_loaded_hashes = set([r.get('transactionHash') for r in receipts1])\n",
    "\n",
    "with open(file2, 'r') as f:\n",
    "    receipts2 = json.load(f)\n",
    "\n",
    "left_txs = []\n",
    "\n",
    "for receipt in receipts2:\n",
    "    tx_hash = receipt.get('transactionHash')\n",
    "    if tx_hash not in already_loaded_hashes:\n",
    "        left_txs.append(receipt)\n",
    "\n",
    "print(f\"Total transactions left: {len(left_txs)}\")\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(left_txs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
