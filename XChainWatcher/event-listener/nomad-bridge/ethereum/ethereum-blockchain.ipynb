{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key():\n",
    "    return os.getenv('ETHEREUM_API_KEY')\n",
    "\n",
    "def get_connection_url(contract_address):\n",
    "    return f'https://svc.blockdaemon.com/universal/v1/ethereum/mainnet/account/{contract_address}/txs'\n",
    "\n",
    "\n",
    "OPTIONS = {\n",
    "    \"headers\": {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-Key\": get_api_key()\n",
    "    }\n",
    "}\n",
    "\n",
    "def extract_transaction_hashes(logs):\n",
    "    return set(log['id'] for log in logs)\n",
    "\n",
    "def save_hashes_to_csv(hashes, filename):\n",
    "    with open(filename, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['hash'])\n",
    "        for tx_hash in hashes:\n",
    "            writer.writerow([tx_hash])\n",
    "\n",
    "def main(ts_start, ts_end, folder_name, contract_address):\n",
    "    pages = 0\n",
    "    total_logs_count = 0\n",
    "    pageToken = None\n",
    "    merged_logs = []\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(f'./data/{folder_name}/'):\n",
    "            os.makedirs(f'./data/{folder_name}/')\n",
    "\n",
    "        while True:\n",
    "            payload = {\n",
    "                \"from\": ts_start,\n",
    "                \"to\": ts_end,\n",
    "                \"order\": 'desc',\n",
    "                \"page_token\": pageToken,\n",
    "                \"page_size\": 100\n",
    "            }\n",
    "        \n",
    "            response = requests.get(get_connection_url(contract_address), headers=OPTIONS['headers'], params=payload)\n",
    "            response_json = response.json()\n",
    "\n",
    "            response_data = response_json['data']\n",
    "            \n",
    "            pages += 1\n",
    "            print(pages)\n",
    "\n",
    "            if (response_json[\"total\"] != 0):\n",
    "                merged_logs.extend(response_data)\n",
    "\n",
    "                if 'meta' not in response_json:\n",
    "                    break\n",
    "\n",
    "                pageToken = response_json[\"meta\"][\"paging\"][\"next_page_token\"]\n",
    "\n",
    "                if pageToken == None:\n",
    "                    break\n",
    "            \n",
    "                total_logs_count += response_json[\"total\"]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        with open(f'./data/{folder_name}/merged_logs.json', 'w') as f:\n",
    "            json.dump(merged_logs, f)\n",
    "\n",
    "        print(f\"Merged all logs into ./data/{folder_name}/merged_logs.json\")\n",
    "        print(f\"Total logs fetched: {total_logs_count}\")\n",
    "\n",
    "        # Extract transaction hashes and save to CSV\n",
    "        transaction_hashes = extract_transaction_hashes(merged_logs)\n",
    "        save_hashes_to_csv(transaction_hashes, f'./data/{folder_name}/transaction_hashes.csv')\n",
    "\n",
    "        print(f\"Transaction hashes saved to ./data/{folder_name}/transaction_hashes.csv\")\n",
    "        print(f\"Total transaction hashes: {len(transaction_hashes)}\")\n",
    "    except Exception as error:\n",
    "        print('Error fetching transactions:', error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevant Timestamps**\n",
    "\n",
    "1641899472 # 11 jan 2022 11:11:12 (deployment of contract 0xd3df in Moonbeam)\n",
    "\n",
    "1671062400 # 15 Dec 2022\n",
    "\n",
    "1722446626 # 31 Jul 2024\n",
    "\n",
    "**Relevant Topics**\n",
    "Deposits: Send(address,address,uint32,bytes32,uint256,bool): 0xa3d219cf126a12be40d7ad1ceef46231c987988dd4e686457b610e1b6b80a4bf\n",
    "\n",
    "Withdrawals: Receive(uint64,address,address,address,uint256): 0x9f9a97db84f39202ca3b409b63f7ccf7d3fd810e176573c7483088b6f181bbbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "Merged all logs into ./data/logs-11Jan-14Dec/merged_logs.json\n",
      "Total logs fetched: 35900\n",
      "Transaction hashes saved to ./data/logs-11Jan-14Dec/transaction_hashes.csv\n",
      "Total transaction hashes: 35967\n"
     ]
    }
   ],
   "source": [
    "contract_address = \"0x88a69b4e698a4b090df6cf5bd7b2d47325ad30a3\"\n",
    "\n",
    "main(1641899472, 1671062400, \"logs-11Jan-14Dec\", contract_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Merged all logs into ./data/logs-14Dec-today/merged_logs.json\n",
      "Total logs fetched: 0\n",
      "Transaction hashes saved to ./data/logs-14Dec-today/transaction_hashes.csv\n",
      "Total transaction hashes: 4\n"
     ]
    }
   ],
   "source": [
    "contract_address = \"0x88a69b4e698a4b090df6cf5bd7b2d47325ad30a3\"    # DOES NOT WORK!!!!\n",
    "\n",
    "main(1671062401, 1722446626, \"logs-14Dec-today\", contract_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction hashes saved to ./data/logs-14Dec-today/transaction_hashes.csv\n",
      "Total transaction hashes: 1779\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FOLDER_NAME = \"logs-14Dec-today\"\n",
    "\n",
    "df1 = pd.read_csv(f'./data/{FOLDER_NAME}/etherscan-0x5d94309e5a0090b165fa4181519701637b6daeba.csv')\n",
    "hashes_df1 = df1['Transaction Hash'].tolist()\n",
    "df2 = pd.read_csv(f'./data/{FOLDER_NAME}/etherscan-0x049b51e531fd8f90da6d92ea83dc4125002f20ef.csv')\n",
    "hashes_df2 = df2['Transaction Hash'].tolist()\n",
    "\n",
    "merged = hashes_df1 + hashes_df2\n",
    "\n",
    "save_hashes_to_csv(merged, f'./data/{FOLDER_NAME}/transaction_hashes.csv')\n",
    "\n",
    "print(f\"Transaction hashes saved to ./data/{FOLDER_NAME}/transaction_hashes.csv\")\n",
    "print(f\"Total transaction hashes: {len(merged)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Transaction Receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "RATE_LIMIT = 20  # requests per second\n",
    "REQUEST_INTERVAL = 1 / RATE_LIMIT\n",
    "\n",
    "CONNECTION_URL = \"https://svc.blockdaemon.com/ethereum/mainnet/native\"\n",
    "\n",
    "def fetch_receipt(tx_hash, folder_name):\n",
    "    data = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"method\": \"eth_getTransactionReceipt\",\n",
    "        \"params\": [tx_hash]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=data)\n",
    "        response_json = response.json()\n",
    "        if 'result' in response_json:\n",
    "            return response_json['result']\n",
    "        else:\n",
    "            print(f\"Error fetching receipt: {response_json}, {tx_hash}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        with open(f\"./data/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving transaction: {tx_hash}, {e}\\n\")\n",
    "\n",
    "def process_hashes(hashes, folder_name):\n",
    "    receipts = []\n",
    "    total_hashes = len(hashes)\n",
    "    progress_interval = max(1, total_hashes // 100)  # Update progress every 1%\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=RATE_LIMIT) as executor:\n",
    "        futures = {executor.submit(fetch_receipt, tx_hash, folder_name): tx_hash for tx_hash in hashes}\n",
    "        \n",
    "        for idx, future in enumerate(as_completed(futures), 1):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                receipts.append(result)\n",
    "\n",
    "                if idx % progress_interval == 0 or idx == total_hashes:\n",
    "                    progress_percentage = (idx / total_hashes) * 100\n",
    "                    print(f\"Progress: {progress_percentage:.2f}% ({idx}/{total_hashes})\")\n",
    "        \n",
    "                time.sleep(REQUEST_INTERVAL)\n",
    "            except Exception as e:\n",
    "                with open(f\"./data/{folder_name}/errors.txt\", \"a\") as error_file:\n",
    "                    error_file.write(f\"Error retrieving transaction: {futures[future]}\\n\")\n",
    "    return receipts\n",
    "\n",
    "def retrieve_receipts(folder_name):\n",
    "    # Read hashes from CSV\n",
    "    hashes_df = pd.read_csv(f'./data/{folder_name}/transaction_hashes.csv')\n",
    "    hashes = hashes_df['hash'].drop_duplicates().tolist()\n",
    "\n",
    "    # Fetch receipts\n",
    "    receipts = process_hashes(hashes, folder_name)\n",
    "\n",
    "    # Save receipts to JSON\n",
    "    with open(f'./data/{folder_name}/tx_receipts.json', 'w') as f:\n",
    "        json.dump(receipts, f)\n",
    "\n",
    "    # Print the count of processed receipts\n",
    "    print(f\"Total receipts fetched: {len(receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.65% (1/154)\n",
      "Progress: 1.30% (2/154)\n",
      "Progress: 1.95% (3/154)\n",
      "Progress: 2.60% (4/154)\n",
      "Progress: 3.25% (5/154)\n",
      "Progress: 3.90% (6/154)\n",
      "Progress: 4.55% (7/154)\n",
      "Progress: 5.19% (8/154)\n",
      "Progress: 5.84% (9/154)\n",
      "Progress: 6.49% (10/154)\n",
      "Progress: 7.14% (11/154)\n",
      "Progress: 7.79% (12/154)\n",
      "Progress: 8.44% (13/154)\n",
      "Progress: 9.09% (14/154)\n",
      "Progress: 9.74% (15/154)\n",
      "Progress: 10.39% (16/154)\n",
      "Progress: 11.04% (17/154)\n",
      "Progress: 11.69% (18/154)\n",
      "Progress: 12.34% (19/154)\n",
      "Progress: 12.99% (20/154)\n",
      "Progress: 13.64% (21/154)\n",
      "Progress: 14.29% (22/154)\n",
      "Progress: 14.94% (23/154)\n",
      "Progress: 15.58% (24/154)\n",
      "Progress: 16.23% (25/154)\n",
      "Progress: 16.88% (26/154)\n",
      "Progress: 17.53% (27/154)\n",
      "Progress: 18.18% (28/154)\n",
      "Progress: 18.83% (29/154)\n",
      "Progress: 19.48% (30/154)\n",
      "Progress: 20.13% (31/154)\n",
      "Progress: 20.78% (32/154)\n",
      "Progress: 21.43% (33/154)\n",
      "Progress: 22.08% (34/154)\n",
      "Progress: 22.73% (35/154)\n",
      "Progress: 23.38% (36/154)\n",
      "Progress: 24.03% (37/154)\n",
      "Progress: 24.68% (38/154)\n",
      "Progress: 25.32% (39/154)\n",
      "Progress: 25.97% (40/154)\n",
      "Progress: 26.62% (41/154)\n",
      "Progress: 27.27% (42/154)\n",
      "Progress: 27.92% (43/154)\n",
      "Progress: 28.57% (44/154)\n",
      "Progress: 29.22% (45/154)\n",
      "Progress: 29.87% (46/154)\n",
      "Progress: 30.52% (47/154)\n",
      "Progress: 31.17% (48/154)\n",
      "Progress: 31.82% (49/154)\n",
      "Progress: 32.47% (50/154)\n",
      "Progress: 33.12% (51/154)\n",
      "Progress: 33.77% (52/154)\n",
      "Progress: 34.42% (53/154)\n",
      "Progress: 35.06% (54/154)\n",
      "Progress: 35.71% (55/154)\n",
      "Progress: 36.36% (56/154)\n",
      "Progress: 37.01% (57/154)\n",
      "Progress: 37.66% (58/154)\n",
      "Progress: 38.31% (59/154)\n",
      "Progress: 38.96% (60/154)\n",
      "Progress: 39.61% (61/154)\n",
      "Progress: 40.26% (62/154)\n",
      "Progress: 40.91% (63/154)\n",
      "Progress: 41.56% (64/154)\n",
      "Progress: 42.21% (65/154)\n",
      "Progress: 42.86% (66/154)\n",
      "Progress: 43.51% (67/154)\n",
      "Progress: 44.16% (68/154)\n",
      "Progress: 44.81% (69/154)\n",
      "Progress: 45.45% (70/154)\n",
      "Progress: 46.10% (71/154)\n",
      "Progress: 46.75% (72/154)\n",
      "Progress: 47.40% (73/154)\n",
      "Progress: 48.05% (74/154)\n",
      "Progress: 48.70% (75/154)\n",
      "Progress: 49.35% (76/154)\n",
      "Progress: 50.00% (77/154)\n",
      "Progress: 50.65% (78/154)\n",
      "Progress: 51.30% (79/154)\n",
      "Progress: 51.95% (80/154)\n",
      "Progress: 52.60% (81/154)\n",
      "Progress: 53.25% (82/154)\n",
      "Progress: 53.90% (83/154)\n",
      "Progress: 54.55% (84/154)\n",
      "Progress: 55.19% (85/154)\n",
      "Progress: 55.84% (86/154)\n",
      "Progress: 56.49% (87/154)\n",
      "Progress: 57.14% (88/154)\n",
      "Progress: 57.79% (89/154)\n",
      "Progress: 58.44% (90/154)\n",
      "Progress: 59.09% (91/154)\n",
      "Progress: 59.74% (92/154)\n",
      "Progress: 60.39% (93/154)\n",
      "Progress: 61.04% (94/154)\n",
      "Progress: 61.69% (95/154)\n",
      "Progress: 62.34% (96/154)\n",
      "Progress: 62.99% (97/154)\n",
      "Progress: 63.64% (98/154)\n",
      "Progress: 64.29% (99/154)\n",
      "Progress: 64.94% (100/154)\n",
      "Progress: 65.58% (101/154)\n",
      "Progress: 66.23% (102/154)\n",
      "Progress: 66.88% (103/154)\n",
      "Progress: 67.53% (104/154)\n",
      "Progress: 68.18% (105/154)\n",
      "Progress: 68.83% (106/154)\n",
      "Progress: 69.48% (107/154)\n",
      "Progress: 70.13% (108/154)\n",
      "Progress: 70.78% (109/154)\n",
      "Progress: 71.43% (110/154)\n",
      "Progress: 72.08% (111/154)\n",
      "Progress: 72.73% (112/154)\n",
      "Progress: 73.38% (113/154)\n",
      "Progress: 74.03% (114/154)\n",
      "Progress: 74.68% (115/154)\n",
      "Progress: 75.32% (116/154)\n",
      "Progress: 75.97% (117/154)\n",
      "Progress: 76.62% (118/154)\n",
      "Progress: 77.27% (119/154)\n",
      "Progress: 77.92% (120/154)\n",
      "Progress: 78.57% (121/154)\n",
      "Progress: 79.22% (122/154)\n",
      "Progress: 79.87% (123/154)\n",
      "Progress: 80.52% (124/154)\n",
      "Progress: 81.17% (125/154)\n",
      "Progress: 81.82% (126/154)\n",
      "Progress: 82.47% (127/154)\n",
      "Progress: 83.12% (128/154)\n",
      "Progress: 83.77% (129/154)\n",
      "Progress: 84.42% (130/154)\n",
      "Progress: 85.06% (131/154)\n",
      "Progress: 85.71% (132/154)\n",
      "Progress: 86.36% (133/154)\n",
      "Progress: 87.01% (134/154)\n",
      "Progress: 87.66% (135/154)\n",
      "Progress: 88.31% (136/154)\n",
      "Progress: 88.96% (137/154)\n",
      "Progress: 89.61% (138/154)\n",
      "Progress: 90.26% (139/154)\n",
      "Progress: 90.91% (140/154)\n",
      "Progress: 91.56% (141/154)\n",
      "Progress: 92.21% (142/154)\n",
      "Progress: 92.86% (143/154)\n",
      "Progress: 93.51% (144/154)\n",
      "Progress: 94.16% (145/154)\n",
      "Progress: 94.81% (146/154)\n",
      "Progress: 95.45% (147/154)\n",
      "Progress: 96.10% (148/154)\n",
      "Progress: 96.75% (149/154)\n",
      "Progress: 97.40% (150/154)\n",
      "Progress: 98.05% (151/154)\n",
      "Progress: 98.70% (152/154)\n",
      "Progress: 99.35% (153/154)\n",
      "Progress: 100.00% (154/154)\n",
      "Total receipts fetched: 154\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (359/35967)\n",
      "Progress: 2.00% (718/35967)\n",
      "Progress: 2.99% (1077/35967)\n",
      "Progress: 3.99% (1436/35967)\n",
      "Progress: 4.99% (1795/35967)\n",
      "Progress: 5.99% (2154/35967)\n",
      "Progress: 6.99% (2513/35967)\n",
      "Progress: 7.99% (2872/35967)\n",
      "Progress: 8.98% (3231/35967)\n",
      "Progress: 9.98% (3590/35967)\n",
      "Progress: 10.98% (3949/35967)\n",
      "Progress: 11.98% (4308/35967)\n",
      "Progress: 12.98% (4667/35967)\n",
      "Progress: 13.97% (5026/35967)\n",
      "Progress: 14.97% (5385/35967)\n",
      "Progress: 15.97% (5744/35967)\n",
      "Progress: 16.97% (6103/35967)\n",
      "Progress: 17.97% (6462/35967)\n",
      "Progress: 18.96% (6821/35967)\n",
      "Progress: 19.96% (7180/35967)\n",
      "Progress: 20.96% (7539/35967)\n",
      "Progress: 21.96% (7898/35967)\n",
      "Progress: 22.96% (8257/35967)\n",
      "Progress: 23.96% (8616/35967)\n",
      "Progress: 24.95% (8975/35967)\n",
      "Progress: 25.95% (9334/35967)\n",
      "Progress: 26.95% (9693/35967)\n",
      "Progress: 27.95% (10052/35967)\n",
      "Progress: 28.95% (10411/35967)\n",
      "Progress: 29.94% (10770/35967)\n",
      "Progress: 30.94% (11129/35967)\n",
      "Progress: 31.94% (11488/35967)\n",
      "Progress: 32.94% (11847/35967)\n",
      "Progress: 33.94% (12206/35967)\n",
      "Progress: 34.93% (12565/35967)\n",
      "Progress: 35.93% (12924/35967)\n",
      "Progress: 36.93% (13283/35967)\n",
      "Progress: 37.93% (13642/35967)\n",
      "Progress: 38.93% (14001/35967)\n",
      "Progress: 39.93% (14360/35967)\n",
      "Progress: 40.92% (14719/35967)\n",
      "Progress: 41.92% (15078/35967)\n",
      "Progress: 42.92% (15437/35967)\n",
      "Progress: 43.92% (15796/35967)\n",
      "Progress: 44.92% (16155/35967)\n",
      "Progress: 45.91% (16514/35967)\n",
      "Progress: 46.91% (16873/35967)\n",
      "Progress: 47.91% (17232/35967)\n",
      "Progress: 48.91% (17591/35967)\n",
      "Progress: 49.91% (17950/35967)\n",
      "Progress: 50.90% (18309/35967)\n",
      "Progress: 51.90% (18668/35967)\n",
      "Progress: 52.90% (19027/35967)\n",
      "Progress: 53.90% (19386/35967)\n",
      "Progress: 54.90% (19745/35967)\n",
      "Progress: 55.90% (20104/35967)\n",
      "Progress: 56.89% (20463/35967)\n",
      "Progress: 57.89% (20822/35967)\n",
      "Progress: 58.89% (21181/35967)\n",
      "Progress: 59.89% (21540/35967)\n",
      "Progress: 60.89% (21899/35967)\n",
      "Progress: 61.88% (22258/35967)\n",
      "Progress: 62.88% (22617/35967)\n",
      "Progress: 63.88% (22976/35967)\n",
      "Progress: 64.88% (23335/35967)\n",
      "Progress: 65.88% (23694/35967)\n",
      "Progress: 66.88% (24053/35967)\n",
      "Progress: 67.87% (24412/35967)\n",
      "Progress: 68.87% (24771/35967)\n",
      "Progress: 69.87% (25130/35967)\n",
      "Progress: 70.87% (25489/35967)\n",
      "Progress: 71.87% (25848/35967)\n",
      "Progress: 72.86% (26207/35967)\n",
      "Progress: 73.86% (26566/35967)\n",
      "Progress: 74.86% (26925/35967)\n",
      "Progress: 75.86% (27284/35967)\n",
      "Progress: 76.86% (27643/35967)\n",
      "Progress: 77.85% (28002/35967)\n",
      "Progress: 78.85% (28361/35967)\n",
      "Progress: 79.85% (28720/35967)\n",
      "Progress: 80.85% (29079/35967)\n",
      "Progress: 81.85% (29438/35967)\n",
      "Progress: 82.85% (29797/35967)\n",
      "Progress: 83.84% (30156/35967)\n",
      "Progress: 84.84% (30515/35967)\n",
      "Progress: 85.84% (30874/35967)\n",
      "Progress: 86.84% (31233/35967)\n",
      "Progress: 87.84% (31592/35967)\n",
      "Progress: 88.83% (31951/35967)\n",
      "Progress: 89.83% (32310/35967)\n",
      "Progress: 90.83% (32669/35967)\n",
      "Progress: 91.83% (33028/35967)\n",
      "Progress: 92.83% (33387/35967)\n",
      "Progress: 93.82% (33746/35967)\n",
      "Progress: 94.82% (34105/35967)\n",
      "Progress: 95.82% (34464/35967)\n",
      "Progress: 96.82% (34823/35967)\n",
      "Progress: 97.82% (35182/35967)\n",
      "Progress: 98.82% (35541/35967)\n",
      "Progress: 99.81% (35900/35967)\n",
      "Progress: 100.00% (35967/35967)\n",
      "Total receipts fetched: 35967\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-11Jan-14Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1.00% (1109/110930)\n",
      "Progress: 2.00% (2218/110930)\n",
      "Progress: 3.00% (3327/110930)\n",
      "Progress: 4.00% (4436/110930)\n",
      "Progress: 5.00% (5545/110930)\n",
      "Progress: 6.00% (6654/110930)\n",
      "Progress: 7.00% (7763/110930)\n",
      "Progress: 8.00% (8872/110930)\n",
      "Progress: 9.00% (9981/110930)\n",
      "Progress: 10.00% (11090/110930)\n",
      "Progress: 11.00% (12199/110930)\n",
      "Progress: 12.00% (13308/110930)\n",
      "Progress: 13.00% (14417/110930)\n",
      "Progress: 14.00% (15526/110930)\n",
      "Progress: 15.00% (16635/110930)\n",
      "Progress: 16.00% (17744/110930)\n",
      "Progress: 17.00% (18853/110930)\n",
      "Progress: 18.00% (19962/110930)\n",
      "Progress: 18.99% (21071/110930)\n",
      "Progress: 19.99% (22180/110930)\n",
      "Progress: 20.99% (23289/110930)\n",
      "Progress: 21.99% (24398/110930)\n",
      "Progress: 22.99% (25507/110930)\n",
      "Progress: 23.99% (26616/110930)\n",
      "Progress: 24.99% (27725/110930)\n",
      "Progress: 25.99% (28834/110930)\n",
      "Progress: 26.99% (29943/110930)\n",
      "Progress: 27.99% (31052/110930)\n",
      "Progress: 28.99% (32161/110930)\n",
      "Progress: 29.99% (33270/110930)\n",
      "Progress: 30.99% (34379/110930)\n",
      "Progress: 31.99% (35488/110930)\n",
      "Progress: 32.99% (36597/110930)\n",
      "Progress: 33.99% (37706/110930)\n",
      "Progress: 34.99% (38815/110930)\n",
      "Progress: 35.99% (39924/110930)\n",
      "Progress: 36.99% (41033/110930)\n",
      "Progress: 37.99% (42142/110930)\n",
      "Progress: 38.99% (43251/110930)\n",
      "Progress: 39.99% (44360/110930)\n",
      "Progress: 40.99% (45469/110930)\n",
      "Progress: 41.99% (46578/110930)\n",
      "Progress: 42.99% (47687/110930)\n",
      "Progress: 43.99% (48796/110930)\n",
      "Progress: 44.99% (49905/110930)\n",
      "Progress: 45.99% (51014/110930)\n",
      "Progress: 46.99% (52123/110930)\n",
      "Progress: 47.99% (53232/110930)\n",
      "Progress: 48.99% (54341/110930)\n",
      "Progress: 49.99% (55450/110930)\n",
      "Progress: 50.99% (56559/110930)\n",
      "Progress: 51.99% (57668/110930)\n",
      "Progress: 52.99% (58777/110930)\n",
      "Progress: 53.99% (59886/110930)\n",
      "Progress: 54.99% (60995/110930)\n",
      "Progress: 55.98% (62104/110930)\n",
      "Progress: 56.98% (63213/110930)\n",
      "Progress: 57.98% (64322/110930)\n",
      "Progress: 58.98% (65431/110930)\n",
      "Progress: 59.98% (66540/110930)\n",
      "Progress: 60.98% (67649/110930)\n",
      "Progress: 61.98% (68758/110930)\n",
      "Progress: 62.98% (69867/110930)\n",
      "Progress: 63.98% (70976/110930)\n",
      "Progress: 64.98% (72085/110930)\n",
      "Progress: 65.98% (73194/110930)\n",
      "Progress: 66.98% (74303/110930)\n",
      "Progress: 67.98% (75412/110930)\n",
      "Progress: 68.98% (76521/110930)\n",
      "Progress: 69.98% (77630/110930)\n",
      "Progress: 70.98% (78739/110930)\n",
      "Progress: 71.98% (79848/110930)\n",
      "Progress: 72.98% (80957/110930)\n",
      "Progress: 73.98% (82066/110930)\n",
      "Progress: 74.98% (83175/110930)\n",
      "Progress: 75.98% (84284/110930)\n",
      "Progress: 76.98% (85393/110930)\n",
      "Progress: 77.98% (86502/110930)\n",
      "Progress: 78.98% (87611/110930)\n",
      "Progress: 79.98% (88720/110930)\n",
      "Progress: 80.98% (89829/110930)\n",
      "Progress: 81.98% (90938/110930)\n",
      "Progress: 82.98% (92047/110930)\n",
      "Progress: 83.98% (93156/110930)\n",
      "Progress: 84.98% (94265/110930)\n",
      "Progress: 85.98% (95374/110930)\n",
      "Progress: 86.98% (96483/110930)\n",
      "Progress: 87.98% (97592/110930)\n",
      "Progress: 88.98% (98701/110930)\n",
      "Progress: 89.98% (99810/110930)\n",
      "Progress: 90.98% (100919/110930)\n",
      "Progress: 91.98% (102028/110930)\n",
      "Progress: 92.97% (103137/110930)\n",
      "Progress: 93.97% (104246/110930)\n",
      "Progress: 94.97% (105355/110930)\n",
      "Progress: 95.97% (106464/110930)\n",
      "Progress: 96.97% (107573/110930)\n",
      "Progress: 97.97% (108682/110930)\n",
      "Progress: 98.97% (109791/110930)\n",
      "Progress: 99.97% (110900/110930)\n",
      "Progress: 100.00% (110930/110930)\n",
      "Total receipts fetched: 110930\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts('logs-28Apr-today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.96% (17/1779)\n",
      "Progress: 1.91% (34/1779)\n",
      "Progress: 2.87% (51/1779)\n",
      "Progress: 3.82% (68/1779)\n",
      "Progress: 4.78% (85/1779)\n",
      "Progress: 5.73% (102/1779)\n",
      "Progress: 6.69% (119/1779)\n",
      "Progress: 7.64% (136/1779)\n",
      "Progress: 8.60% (153/1779)\n",
      "Progress: 9.56% (170/1779)\n",
      "Progress: 10.51% (187/1779)\n",
      "Progress: 11.47% (204/1779)\n",
      "Progress: 12.42% (221/1779)\n",
      "Progress: 13.38% (238/1779)\n",
      "Progress: 14.33% (255/1779)\n",
      "Progress: 15.29% (272/1779)\n",
      "Progress: 16.25% (289/1779)\n",
      "Progress: 17.20% (306/1779)\n",
      "Progress: 18.16% (323/1779)\n",
      "Progress: 19.11% (340/1779)\n",
      "Progress: 20.07% (357/1779)\n",
      "Progress: 21.02% (374/1779)\n",
      "Progress: 21.98% (391/1779)\n",
      "Progress: 22.93% (408/1779)\n",
      "Progress: 23.89% (425/1779)\n",
      "Progress: 24.85% (442/1779)\n",
      "Progress: 25.80% (459/1779)\n",
      "Progress: 26.76% (476/1779)\n",
      "Progress: 27.71% (493/1779)\n",
      "Progress: 28.67% (510/1779)\n",
      "Progress: 29.62% (527/1779)\n",
      "Progress: 30.58% (544/1779)\n",
      "Progress: 31.53% (561/1779)\n",
      "Progress: 32.49% (578/1779)\n",
      "Progress: 33.45% (595/1779)\n",
      "Progress: 34.40% (612/1779)\n",
      "Progress: 35.36% (629/1779)\n",
      "Progress: 36.31% (646/1779)\n",
      "Progress: 37.27% (663/1779)\n",
      "Progress: 38.22% (680/1779)\n",
      "Progress: 39.18% (697/1779)\n",
      "Progress: 40.13% (714/1779)\n",
      "Progress: 41.09% (731/1779)\n",
      "Progress: 42.05% (748/1779)\n",
      "Progress: 43.00% (765/1779)\n",
      "Progress: 43.96% (782/1779)\n",
      "Progress: 44.91% (799/1779)\n",
      "Progress: 45.87% (816/1779)\n",
      "Progress: 46.82% (833/1779)\n",
      "Progress: 47.78% (850/1779)\n",
      "Progress: 48.74% (867/1779)\n",
      "Progress: 49.69% (884/1779)\n",
      "Progress: 50.65% (901/1779)\n",
      "Progress: 51.60% (918/1779)\n",
      "Progress: 52.56% (935/1779)\n",
      "Progress: 53.51% (952/1779)\n",
      "Progress: 54.47% (969/1779)\n",
      "Progress: 55.42% (986/1779)\n",
      "Progress: 56.38% (1003/1779)\n",
      "Progress: 57.34% (1020/1779)\n",
      "Progress: 58.29% (1037/1779)\n",
      "Progress: 59.25% (1054/1779)\n",
      "Progress: 60.20% (1071/1779)\n",
      "Progress: 61.16% (1088/1779)\n",
      "Progress: 62.11% (1105/1779)\n",
      "Progress: 63.07% (1122/1779)\n",
      "Progress: 64.02% (1139/1779)\n",
      "Progress: 64.98% (1156/1779)\n",
      "Progress: 65.94% (1173/1779)\n",
      "Progress: 66.89% (1190/1779)\n",
      "Progress: 67.85% (1207/1779)\n",
      "Progress: 68.80% (1224/1779)\n",
      "Progress: 69.76% (1241/1779)\n",
      "Progress: 70.71% (1258/1779)\n",
      "Progress: 71.67% (1275/1779)\n",
      "Progress: 72.63% (1292/1779)\n",
      "Progress: 73.58% (1309/1779)\n",
      "Progress: 74.54% (1326/1779)\n",
      "Progress: 75.49% (1343/1779)\n",
      "Progress: 76.45% (1360/1779)\n",
      "Progress: 77.40% (1377/1779)\n",
      "Progress: 78.36% (1394/1779)\n",
      "Progress: 79.31% (1411/1779)\n",
      "Progress: 80.27% (1428/1779)\n",
      "Progress: 81.23% (1445/1779)\n",
      "Progress: 82.18% (1462/1779)\n",
      "Progress: 83.14% (1479/1779)\n",
      "Progress: 84.09% (1496/1779)\n",
      "Progress: 85.05% (1513/1779)\n",
      "Progress: 86.00% (1530/1779)\n",
      "Progress: 86.96% (1547/1779)\n",
      "Progress: 87.91% (1564/1779)\n",
      "Progress: 88.87% (1581/1779)\n",
      "Progress: 89.83% (1598/1779)\n",
      "Progress: 90.78% (1615/1779)\n",
      "Progress: 91.74% (1632/1779)\n",
      "Progress: 92.69% (1649/1779)\n",
      "Progress: 93.65% (1666/1779)\n",
      "Progress: 94.60% (1683/1779)\n",
      "Progress: 95.56% (1700/1779)\n",
      "Progress: 96.51% (1717/1779)\n",
      "Progress: 97.47% (1734/1779)\n",
      "Progress: 98.43% (1751/1779)\n",
      "Progress: 99.38% (1768/1779)\n",
      "Progress: 100.00% (1779/1779)\n",
      "Total receipts fetched: 1779\n"
     ]
    }
   ],
   "source": [
    "retrieve_receipts(\"logs-14Dec-today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def load_failed_hashes(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        hashes = [re.findall(pattern='0x[a-fA-F0-9]{64}', string=line)[0] for line in f]\n",
    "        print(hashes)\n",
    "    return hashes\n",
    "\n",
    "def retrieve_failed_receipts(folder_name):\n",
    "    errors_file = f'./data/{folder_name}/errors.txt'\n",
    "    if not os.path.exists(errors_file):\n",
    "        print(f\"Error file {errors_file} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Load failed transaction hashes from errors.txt\n",
    "    failed_hashes = load_failed_hashes(f'./data/{folder_name}/errors.txt')\n",
    "\n",
    "    # Fetch receipts for failed hashes\n",
    "    failed_receipts = process_hashes(failed_hashes, folder_name)\n",
    "\n",
    "    # Save failed receipts to JSON\n",
    "    with open(f'./data/{folder_name}/tx_receipts_2.json', 'w') as f:\n",
    "        json.dump(failed_receipts, f)\n",
    "\n",
    "    # Print the count of processed failed receipts\n",
    "    print(f\"Total failed receipts fetched: {len(failed_receipts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0x02ea3831a14936046472fdc74a8377e2d31951ff39dea146edb59c80750b7561']\n",
      "Progress: 100.00% (1/1)\n",
      "Total failed receipts fetched: 1\n"
     ]
    }
   ],
   "source": [
    "retrieve_failed_receipts('logs-11Jan-14Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second interval, we need to merge the json files with transaction receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_json_files(folder_name):\n",
    "    file1 = f'./data/{folder_name}/tx_receipts.json'\n",
    "    file2 = f'./data/{folder_name}/tx_receipts_2.json'\n",
    "    output_file = f'./data/{folder_name}/merged_tx_receipts.json'\n",
    "\n",
    "    with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        json1 = json.load(f1)\n",
    "        json2 = json.load(f2)\n",
    "\n",
    "    merged_json = json1 + json2\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(merged_json, f)\n",
    "\n",
    "    print(f\"Number of transaction receipts in {file1}: {len(json1)}\")\n",
    "    print(f\"Number of transaction receipts in {file2}: {len(json2)}\")\n",
    "    print(f\"Number of transaction receipts in the merged file: {len(merged_json)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/tx_receipts.json: 35967\n",
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/tx_receipts_2.json: 1\n",
      "Number of transaction receipts in the merged file: 35968\n"
     ]
    }
   ],
   "source": [
    "merge_json_files(\"logs-11Jan-14Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from json file with result field of receipts\n",
    "def remove_duplicates(file, folder_name):\n",
    "    with open(file, 'r') as f:\n",
    "        receipts = json.load(f)\n",
    "\n",
    "    unique_receipts = []\n",
    "    unique_hashes = set()\n",
    "\n",
    "    for receipt in receipts:\n",
    "        try:\n",
    "            tx_hash = receipt.get('transactionHash')\n",
    "            if tx_hash not in unique_hashes:\n",
    "                unique_hashes.add(tx_hash)\n",
    "                unique_receipts.append(receipt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing receipt: {receipt}, {e}\")\n",
    "\n",
    "    # Save unique receipts to a new file\n",
    "    unique_output_file = f'./data/{folder_name}/unique_tx_receipts-new-2.json'\n",
    "    with open(unique_output_file, 'w') as f:\n",
    "        json.dump(unique_receipts, f)\n",
    "\n",
    "    print(f\"Unique receipts saved to {unique_output_file}\")\n",
    "\n",
    "    print(f\"Removed {len(receipts) - len(unique_receipts)} duplicate receipts\")\n",
    "    print(f\"Total receipts: {len(receipts)}\")\n",
    "    print(f\"Total unique receipts: {len(unique_receipts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing receipt: None, 'NoneType' object has no attribute 'get'\n",
      "Unique receipts saved to ./data/logs-11Jan-14Dec/unique_tx_receipts.json\n",
      "Removed 1 duplicate receipts\n",
      "Total receipts: 35968\n",
      "Total unique receipts: 35967\n"
     ]
    }
   ],
   "source": [
    "tx_receipts = './data/logs-11Jan-14Dec/merged_tx_receipts.json'\n",
    "\n",
    "remove_duplicates(tx_receipts, \"logs-11Jan-14Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-14Dec-today/unique_tx_receipts.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 1779\n",
      "Total unique receipts: 1779\n"
     ]
    }
   ],
   "source": [
    "tx_receipts = './data/logs-14Dec-today/tx_receipts.json'\n",
    "\n",
    "remove_duplicates(tx_receipts, \"logs-14Dec-today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_api_key():\n",
    "    return os.getenv('ETHEREUM_API_KEY')\n",
    "\n",
    "CONNECTION_URL = \"https://svc.blockdaemon.com/ethereum/mainnet/native\"\n",
    "\n",
    "\n",
    "OPTIONS = {\n",
    "    \"headers\": {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"X-API-Key\": get_api_key()\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_block_data(block_number, errors_file):\n",
    "    payload = {\n",
    "        \"id\": 1,\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"params\": [],\n",
    "        \"method\": \"eth_getBlockByNumber\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        payload['params'] = [block_number, False]\n",
    "        response = requests.post(CONNECTION_URL, headers=OPTIONS['headers'], json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            block = response.json()[\"result\"]\n",
    "            block_no = int(block[\"number\"], 16)\n",
    "            timestamp = int(block[\"timestamp\"], 16)\n",
    "            transactions = len(block[\"transactions\"])\n",
    "            return f\"{block_no},{transactions},{timestamp}\\n\"\n",
    "        else:\n",
    "            with open(errors_file, \"a\") as error_file:\n",
    "                error_file.write(f\"Error code: {block_number}\\n\")\n",
    "            return \"null,null,null\\n\"\n",
    "    except Exception as e:\n",
    "        with open(errors_file, \"a\") as error_file:\n",
    "            error_file.write(f\"Error retrieving block: {block_number}, {e}\\n\")\n",
    "        return \"null,null,null\\n\"\n",
    "\n",
    "def get_blocks_data(folder_name):\n",
    "    input_file = f'./data/{folder_name}/tx_receipts.json'\n",
    "    output_file = f'./data/{folder_name}/blocks.csv'\n",
    "    errors_file = f'./data/{folder_name}/errors.txt'\n",
    "\n",
    "    with open(input_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    print(f\"Extracting block number and Unix timestamp from {len(tx_receipts)} transaction receipts...\")\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts]\n",
    "\n",
    "    print(f\"Extracted {len(block_numbers)} block numbers...\")\n",
    "\n",
    "    with open(output_file, \"a\") as blocks_file:\n",
    "        blocks_file.write(\"block_number,transactions,timestamp\\n\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            # Submit tasks for each block in the range\n",
    "            futures = {executor.submit(get_block_data, block_number, errors_file): block_number for block_number in block_numbers if block_number is not None}\n",
    "\n",
    "            # Process the completed tasks and write to the file\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                block_number = futures[future]\n",
    "                try:\n",
    "                    blocks_data = future.result()\n",
    "                    blocks_file.write(blocks_data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing block {block_number}: {e}\")\n",
    "\n",
    "    print(f'Extracted block number and Unix timestamp to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 35967 transaction receipts...\n",
      "Extracted 35967 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data/logs-11Jan-14Dec/blocks_2.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-11Jan-14Dec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 1779 transaction receipts...\n",
      "Extracted 1779 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data/logs-14Dec-today/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-14Dec-today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_missing_blocks(folder_name):\n",
    "    blocks_file = f'./data/{folder_name}/blocks.csv'\n",
    "    receipts_file = f'./data/{folder_name}/tx_receipts.json'\n",
    "    errors_file = f'./data/{folder_name}/errors_2.txt'\n",
    "\n",
    "    blocks_data = pd.read_csv(blocks_file)\n",
    "\n",
    "    with open(receipts_file, 'r') as file:\n",
    "        tx_receipts = json.load(file)\n",
    "\n",
    "    block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts if tx is not None]\n",
    "    \n",
    "    print(\"Loaded block numbers and block data...\")\n",
    "\n",
    "    print(len(block_numbers))\n",
    "    print(len(blocks_data['block_number']))\n",
    "    \n",
    "    missing_blocks = set(block_numbers) - set(hex(block) for block in blocks_data['block_number'] if block is not None)\n",
    "\n",
    "    print(f\"Total missing blocks: {len(missing_blocks)}\")\n",
    "\n",
    "    with open(blocks_file, 'a') as blocks_file:\n",
    "        for missing_block in missing_blocks:\n",
    "            line = get_block_data(missing_block, errors_file)\n",
    "            blocks_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "35966\n",
      "35965\n",
      "Total missing blocks: 1\n"
     ]
    }
   ],
   "source": [
    "retrieve_missing_blocks('logs-11Jan-14Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, there was data being left out when using the Blockdaemon's Universal API. So we extracted transactions manually \n",
    "from Etherscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "txs = pd.read_csv('./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/aggregated.csv', sep=',')\n",
    "\n",
    "txs = txs[txs['Status'] == 'Success']\n",
    "\n",
    "txs.to_csv('./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/successful_txs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8922\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/tx_receipts.json\", 'r') as f1:\n",
    "    json1 = json.load(f1)\n",
    "\n",
    "new_receipts = pd.read_csv(\"./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/successful_txs.csv\")\n",
    "\n",
    "new_json = []\n",
    "\n",
    "for tx_receipt in json1:\n",
    "    if tx_receipt['transactionHash'] in new_receipts['Transaction Hash'].tolist():\n",
    "        new_json.append(tx_receipt)\n",
    "\n",
    "print(len(new_json))\n",
    "\n",
    "with open(\"./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/successful_txs_receipts.json\", \"w\") as f:\n",
    "    json.dump(new_json, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/unique_tx_receipts.json: 35967\n",
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/successful_txs_receipts.json: 8922\n",
      "Number of transaction receipts in the merged file: 44889\n"
     ]
    }
   ],
   "source": [
    "# merge json files\n",
    "\n",
    "file1 = './data/logs-11Jan-14Dec/unique_tx_receipts.json'\n",
    "file2 = './data/logs-11Jan-14Dec/missing-data-with-native-eth/0x049b51e531fd8f90da6d92ea83dc4125002f20ef/successful_txs_receipts.json'\n",
    "output_file = './data/logs-11Jan-14Dec/unique_tx_receipts-2.json'\n",
    "\n",
    "with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "    json1 = json.load(f1)\n",
    "    json2 = json.load(f2)\n",
    "\n",
    "merged_json = json1 + json2\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(merged_json, f)\n",
    "\n",
    "print(f\"Number of transaction receipts in {file1}: {len(json1)}\")\n",
    "print(f\"Number of transaction receipts in {file2}: {len(json2)}\")\n",
    "print(f\"Number of transaction receipts in the merged file: {len(merged_json)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-11Jan-14Dec/unique_tx_receipts-new.json\n",
      "Removed 4229 duplicate receipts\n",
      "Total receipts: 44889\n",
      "Total unique receipts: 40660\n"
     ]
    }
   ],
   "source": [
    "input_file = './data/logs-11Jan-14Dec/unique_tx_receipts-2.json'\n",
    "remove_duplicates(input_file, \"logs-11Jan-14Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40660\n",
      "35967\n",
      "4693\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/logs-11Jan-14Dec/unique_tx_receipts-new.json\", 'r') as f1, open(file1, 'r') as f2:\n",
    "    json1 = json.load(f1)\n",
    "    json2 = json.load(f2)\n",
    "\n",
    "\n",
    "print(len(json1))\n",
    "print(len(json2))\n",
    "\n",
    "newly_added_tx_receipts = [tx_receipt for tx_receipt in json1 if tx_receipt not in json2]\n",
    "print(len(newly_added_tx_receipts))\n",
    "\n",
    "with open(\"./data/logs-11Jan-14Dec/missing-txs.json\", 'w') as f:\n",
    "    json.dump(newly_added_tx_receipts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded block numbers and block data...\n",
      "4693\n",
      "35966\n",
      "Total missing blocks: 4376\n"
     ]
    }
   ],
   "source": [
    "blocks_file = f'./data/logs-11Jan-14Dec/blocks.csv'\n",
    "receipts_file = f'./data/logs-11Jan-14Dec/missing-txs.json'\n",
    "errors_file = f'./data/logs-11Jan-14Dec/errors_2.txt'\n",
    "\n",
    "blocks_data = pd.read_csv(blocks_file)\n",
    "\n",
    "with open(receipts_file, 'r') as file:\n",
    "    tx_receipts = json.load(file)\n",
    "\n",
    "block_numbers = [tx[\"blockNumber\"] for tx in tx_receipts if tx is not None]\n",
    "\n",
    "print(\"Loaded block numbers and block data...\")\n",
    "\n",
    "print(len(block_numbers))\n",
    "print(len(blocks_data['block_number']))\n",
    "\n",
    "missing_blocks = set(block_numbers) - set(hex(block) for block in blocks_data['block_number'] if block is not None)\n",
    "\n",
    "print(f\"Total missing blocks: {len(missing_blocks)}\")\n",
    "\n",
    "with open(blocks_file, 'a') as blocks_file:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Submit tasks for each block in the range\n",
    "        futures = {executor.submit(get_block_data, missing_block, errors_file): missing_block for missing_block in missing_blocks}\n",
    "\n",
    "        # Process the completed tasks and write to the file\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            block_number = futures[future]\n",
    "            try:\n",
    "                blocks_data = future.result()\n",
    "                blocks_file.write(blocks_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing block {block_number}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is still missing data for deposits of WGLMR. So we extracted transactions manually from Etherscan as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "txs = pd.read_csv('./data/logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663/aggregated.csv', sep=',')\n",
    "\n",
    "txs = txs[txs['Status'] == 'Success']\n",
    "\n",
    "txs.to_csv('./data/logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663/successful_txs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/unique_tx_receipts-new.json: 40660\n",
      "Number of transaction receipts in ./data/logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663/tx_receipts.json: 154\n",
      "Number of transaction receipts in the merged file: 40814\n"
     ]
    }
   ],
   "source": [
    "# merge json files\n",
    "\n",
    "file1 = './data/logs-11Jan-14Dec/unique_tx_receipts-new.json'\n",
    "file2 = './data/logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663/tx_receipts.json'\n",
    "output_file = './data/logs-11Jan-14Dec/unique_tx_receipts-3.json'\n",
    "\n",
    "with open(file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "    json1 = json.load(f1)\n",
    "    json2 = json.load(f2)\n",
    "\n",
    "merged_json = json1 + json2\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(merged_json, f)\n",
    "\n",
    "print(f\"Number of transaction receipts in {file1}: {len(json1)}\")\n",
    "print(f\"Number of transaction receipts in {file2}: {len(json2)}\")\n",
    "print(f\"Number of transaction receipts in the merged file: {len(merged_json)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique receipts saved to ./data/logs-11Jan-14Dec/unique_tx_receipts-new-2.json\n",
      "Removed 0 duplicate receipts\n",
      "Total receipts: 40814\n",
      "Total unique receipts: 40814\n"
     ]
    }
   ],
   "source": [
    "input = './data/logs-11Jan-14Dec/unique_tx_receipts-3.json'\n",
    "remove_duplicates(input, \"logs-11Jan-14Dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting block number and Unix timestamp from 154 transaction receipts...\n",
      "Extracted 154 block numbers...\n",
      "Extracted block number and Unix timestamp to ./data/logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663/blocks.csv\n"
     ]
    }
   ],
   "source": [
    "get_blocks_data('logs-11Jan-14Dec/missing-data-wglmr/0xba8d75baccc4d5c4bd814fde69267213052ea663')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
